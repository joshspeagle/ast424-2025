{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Example: Galaxy Morphology Classification with Deep Learning\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load galaxy images from the Galaxy10 DECaLS dataset\n",
    "2. Build a Random Forest baseline with hand-crafted features\n",
    "3. Build a simple CNN classifier with PyTorch\n",
    "4. Compare performance between traditional and deep learning approaches\n",
    "5. Visualize classification results and errors\n",
    "\n",
    "## Research Applications\n",
    "- Automated galaxy classification for large surveys\n",
    "- Discovering rare galaxy types\n",
    "- Studying galaxy evolution through morphology\n",
    "- Preparing for LSST and Euclid surveys\n",
    "\n",
    "**Expected runtime:** ~5-10 minutes (CPU), ~2-3 minutes (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn for baseline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "print(\"✅ Scikit-learn available\")\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "print(\"✅ PyTorch available\")\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ No GPU available, using CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Motivation\n",
    "\n",
    "### Why Galaxy Morphology Matters\n",
    "\n",
    "Galaxy morphology reveals crucial information about:\n",
    "1. **Formation history and evolution** - How galaxies form and grow\n",
    "2. **Dark matter content** - Distribution of visible and dark matter\n",
    "3. **Star formation rates** - Active vs quenched galaxies\n",
    "4. **Environmental effects** - Impact of galaxy interactions\n",
    "\n",
    "### Traditional Classification Challenges\n",
    "- Visual classification is subjective and slow\n",
    "- Billions of galaxies in upcoming surveys (LSST, Euclid)\n",
    "- Need consistent, reproducible classifications\n",
    "\n",
    "### Deep Learning Advantages\n",
    "- Can process millions of galaxies quickly\n",
    "- Learns complex morphological features\n",
    "- Provides uncertainty estimates\n",
    "- Can discover new/rare galaxy types\n",
    "\n",
    "### Galaxy10 DECaLS Dataset\n",
    "- 17,736 labeled galaxy images\n",
    "- 10 morphological classes\n",
    "- 256×256 pixel RGB images\n",
    "- Based on Galaxy Zoo classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Galaxy10 DECaLS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading gz10 dataset (full training set)...\")\n",
    "# Load full dataset: 17,736 galaxies\n",
    "gz10_data = load_dataset(\n",
    "    \"MultimodalUniverse/gz10\",\n",
    "    split=\"train\",\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded {len(gz10_data)} galaxy images\")\n",
    "\n",
    "# Galaxy class names\n",
    "class_names = [\n",
    "    'Smooth Round',      # 0\n",
    "    'Smooth In-between', # 1\n",
    "    'Smooth Cigar',      # 2\n",
    "    'Edge-on Disk',      # 3\n",
    "    'Spiral Barred',     # 4\n",
    "    'Spiral Not Barred', # 5\n",
    "    'Irregular',         # 6\n",
    "    'Merger',           # 7\n",
    "    'Disturbed',        # 8\n",
    "    'Other'             # 9\n",
    "]\n",
    "\n",
    "# Examine data structure\n",
    "sample = gz10_data[0]\n",
    "print(f\"\\nSample keys: {list(sample.keys())}\")\n",
    "print(f\"Image type: {type(sample['rgb_image'])}\")\n",
    "print(f\"Label: {sample['gz10_label']} ({class_names[sample['gz10_label']]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "labels = [gz10_data[i]['gz10_label'] for i in range(len(gz10_data))]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(\"Class distribution in dataset:\")\n",
    "print(\"-\" * 40)\n",
    "for cls, count in zip(unique, counts):\n",
    "    percentage = 100 * count / len(labels)\n",
    "    print(f\"{class_names[cls]:20s}: {count:4d} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample galaxies\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"Creating galaxy montage...\")\n",
    "for i in range(15):\n",
    "    img = gz10_data[i]['rgb_image']\n",
    "    label = gz10_data[i]['gz10_label']\n",
    "    \n",
    "    # Convert PIL to numpy if needed\n",
    "    if hasattr(img, 'convert'):\n",
    "        img = np.array(img)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{class_names[label]}\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Galaxies from gz10 Dataset', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_sample_galaxies.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved galaxy samples to gz10_sample_galaxies.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to arrays\n",
    "print(\"Converting images to arrays...\")\n",
    "images = []\n",
    "labels_array = []\n",
    "\n",
    "for i in range(len(gz10_data)):\n",
    "    img = gz10_data[i]['rgb_image']\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    if hasattr(img, 'convert'):\n",
    "        img_array = np.array(img)\n",
    "    else:\n",
    "        img_array = img\n",
    "    \n",
    "    images.append(img_array)\n",
    "    labels_array.append(gz10_data[i]['gz10_label'])\n",
    "\n",
    "images = np.array(images)\n",
    "labels_array = np.array(labels_array)\n",
    "\n",
    "print(f\"Image array shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels_array.shape}\")\n",
    "\n",
    "# Split into train/validation sets (80/20)\n",
    "n_samples = len(images)\n",
    "n_train = int(0.8 * n_samples)\n",
    "indices = np.random.permutation(n_samples)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx = indices[n_train:]\n",
    "\n",
    "X_train = images[train_idx]\n",
    "y_train = labels_array[train_idx]\n",
    "X_val = images[val_idx]\n",
    "y_val = labels_array[val_idx]\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} images\")\n",
    "print(f\"Validation set: {len(X_val)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Classification with Random Forest\n",
    "\n",
    "Before using deep learning, let's establish a baseline using traditional machine learning with hand-crafted features:\n",
    "- Color statistics (mean and std in RGB)\n",
    "- Concentration (center vs outer brightness)\n",
    "- Asymmetry (left-right symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting simple image features...\")\n",
    "\n",
    "# Extract simple features from images\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # Basic statistics\n",
    "        mean_rgb = np.mean(img, axis=(0, 1))\n",
    "        std_rgb = np.std(img, axis=(0, 1))\n",
    "        \n",
    "        # Central vs outer brightness\n",
    "        h, w = img.shape[:2]\n",
    "        center = img[h//4:3*h//4, w//4:3*w//4]\n",
    "        outer = np.concatenate([\n",
    "            img[:h//4, :].flatten(),\n",
    "            img[3*h//4:, :].flatten(),\n",
    "            img[:, :w//4].flatten(),\n",
    "            img[:, 3*w//4:].flatten()\n",
    "        ])\n",
    "        \n",
    "        center_bright = np.mean(center)\n",
    "        outer_bright = np.mean(outer) if len(outer) > 0 else 0\n",
    "        concentration = center_bright / (outer_bright + 1e-6)\n",
    "        \n",
    "        # Asymmetry (simple)\n",
    "        left_half = img[:, :w//2]\n",
    "        right_half = img[:, w//2:]\n",
    "        asymmetry = np.mean(np.abs(left_half - np.fliplr(right_half)))\n",
    "        \n",
    "        # Combine features\n",
    "        feat = np.concatenate([\n",
    "            mean_rgb,\n",
    "            std_rgb,\n",
    "            [concentration, asymmetry]\n",
    "        ])\n",
    "        features.append(feat)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features\n",
    "X_train_feat = extract_features(X_train)\n",
    "X_val_feat = extract_features(X_val)\n",
    "\n",
    "print(f\"Feature shape: {X_train_feat.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "X_val_scaled = scaler.transform(X_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest classifier...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_train_acc = rf.score(X_train_scaled, y_train) * 100\n",
    "rf_val_acc = rf.score(X_val_scaled, y_val) * 100\n",
    "\n",
    "print(f\"Training Accuracy: {rf_train_acc:.2f}%\")\n",
    "print(f\"Validation Accuracy: {rf_val_acc:.2f}%\")\n",
    "\n",
    "# Get predictions for later comparison\n",
    "rf_predictions = rf.predict(X_val_scaled)\n",
    "rf_pred_proba = rf.predict_proba(X_val_scaled)\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['R_mean', 'G_mean', 'B_mean',\n",
    "                'R_std', 'G_std', 'B_std',\n",
    "                'Concentration', 'Asymmetry']\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"  {feature_names[indices[i]]:15s}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Random Forest\n",
    "print(\"Creating Random Forest visualizations...\")\n",
    "rf_cm = confusion_matrix(y_val, rf_predictions)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Confusion Matrix\n",
    "ax = axes[0]\n",
    "im = ax.imshow(rf_cm, cmap='Blues')\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_yticks(np.arange(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(class_names, fontsize=8)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax.text(j, i, rf_cm[i, j],\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if rf_cm[i, j] > rf_cm.max() / 2 else \"black\",\n",
    "                     fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "ax.set_ylabel('True Label', fontsize=10)\n",
    "ax.set_title(f'Random Forest Confusion Matrix\\nValidation Accuracy: {rf_val_acc:.2f}%', fontsize=11)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 2: Feature Importances\n",
    "ax = axes[1]\n",
    "ax.barh(range(len(feature_names)), importances[indices])\n",
    "ax.set_yticks(range(len(feature_names)))\n",
    "ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Random Forest Feature Importances')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_rf_performance.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved Random Forest performance to gz10_rf_performance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for Random Forest (one-vs-rest)\n",
    "print(\"Calculating ROC curves...\")\n",
    "y_val_bin = label_binarize(y_val, classes=range(10))\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "fpr_rf = dict()\n",
    "tpr_rf = dict()\n",
    "roc_auc_rf = dict()\n",
    "\n",
    "for i in range(10):\n",
    "    fpr_rf[i], tpr_rf[i], _ = roc_curve(y_val_bin[:, i], rf_pred_proba[:, i])\n",
    "    roc_auc_rf[i] = auc(fpr_rf[i], tpr_rf[i])\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr_rf[\"micro\"], tpr_rf[\"micro\"], _ = roc_curve(y_val_bin.ravel(), rf_pred_proba.ravel())\n",
    "roc_auc_rf[\"micro\"] = auc(fpr_rf[\"micro\"], tpr_rf[\"micro\"])\n",
    "\n",
    "print(f\"Random Forest Micro-average AUC: {roc_auc_rf['micro']:.3f}\")\n",
    "print(\"\\n✅ Baseline Random Forest complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Learning Classification with PyTorch\n",
    "\n",
    "Now let's build a Convolutional Neural Network (CNN) that can learn features directly from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to PIL for transforms\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image.astype('uint8'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GalaxyDataset(X_train, y_train, transform=transform_train)\n",
    "val_dataset = GalaxyDataset(X_val, y_val, transform=transform_val)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✅ Created data loaders (batch size: {batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN Architecture (lightweight for fast training)\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight CNN with aggressive pooling.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1: 3→16 channels, 3×3 MaxPool (stride 3) → 85×85×16\n",
    "    - Conv2: 16→24 channels, 3×3 MaxPool (stride 3) → 28×28×24\n",
    "    - Conv3: 24→32 channels, 3×3 MaxPool (stride 3) → 9×9×32\n",
    "    - Flatten: 2,592 features\n",
    "    - FC1: 2,592→40\n",
    "    - FC2: 40→10\n",
    "    \n",
    "    Total parameters: ~115K\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(24, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Aggressive 3×3 pooling with stride 3\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 3 pools of stride 3: 256 → 85 → 28 → 9\n",
    "        # Feature map size: 9×9×32 = 2,592\n",
    "        self.fc1 = nn.Linear(32 * 9 * 9, 40)\n",
    "        self.fc2 = nn.Linear(40, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1: 256×256×3 → 85×85×16\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        # Conv block 2: 85×85×16 → 28×28×24\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        # Conv block 3: 28×28×24 → 9×9×32\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten: 9×9×32 → 2,592\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "print(\"Building CNN model...\")\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return running_loss / len(dataloader), accuracy\n",
    "\n",
    "def validate(model, dataloader, criterion, device, return_probs=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Validating\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities for ROC curves\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    if return_probs:\n",
    "        return running_loss / len(dataloader), accuracy, all_predictions, all_labels, all_probs\n",
    "    return running_loss / len(dataloader), accuracy, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training CNN model (lightweight architecture)...\")\n",
    "n_epochs = 20  # Train for 20 epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"Final Model Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "_, final_acc, predictions, true_labels, cnn_pred_proba = validate(\n",
    "    model, val_loader, criterion, device, return_probs=True\n",
    ")\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "cnn_pred_proba = np.array(cnn_pred_proba)\n",
    "print(f\"Final Validation Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "# Compute ROC curves for CNN\n",
    "print(\"\\nCalculating CNN ROC curves...\")\n",
    "y_val_bin_cnn = label_binarize(true_labels, classes=range(10))\n",
    "\n",
    "fpr_cnn = dict()\n",
    "tpr_cnn = dict()\n",
    "roc_auc_cnn = dict()\n",
    "\n",
    "for i in range(10):\n",
    "    fpr_cnn[i], tpr_cnn[i], _ = roc_curve(y_val_bin_cnn[:, i], cnn_pred_proba[:, i])\n",
    "    roc_auc_cnn[i] = auc(fpr_cnn[i], tpr_cnn[i])\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr_cnn[\"micro\"], tpr_cnn[\"micro\"], _ = roc_curve(y_val_bin_cnn.ravel(), cnn_pred_proba.ravel())\n",
    "roc_auc_cnn[\"micro\"] = auc(fpr_cnn[\"micro\"], tpr_cnn[\"micro\"])\n",
    "\n",
    "print(f\"CNN Micro-average AUC: {roc_auc_cnn['micro']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Train')\n",
    "ax1.plot(val_losses, label='Validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, label='Train')\n",
    "ax2.plot(val_accs, label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CNN Training History')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_training_history.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved training history\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_yticks(np.arange(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax.text(j, i, cm[i, j],\n",
    "                     ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "final_val_acc = 100 * np.sum(predictions == true_labels) / len(true_labels)\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_title(f'CNN Confusion Matrix\\nValidation Accuracy: {final_val_acc:.2f}%', fontsize=12)\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Random Forest vs CNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves comparison\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "for i in range(10):\n",
    "    ax = axes[i]\n",
    "    ax.plot(fpr_rf[i], tpr_rf[i], label=f'RF (AUC = {roc_auc_rf[i]:.2f})', linewidth=2)\n",
    "    ax.plot(fpr_cnn[i], tpr_cnn[i], label=f'CNN (AUC = {roc_auc_cnn[i]:.2f})', linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'{class_names[i]}', fontsize=10)\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ROC Curves: Random Forest vs CNN (per class)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved ROC curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot micro-average ROC curve\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr_rf[\"micro\"], tpr_rf[\"micro\"],\n",
    "        label=f'RF (AUC = {roc_auc_rf[\"micro\"]:.3f})', linewidth=2, color='blue')\n",
    "ax.plot(fpr_cnn[\"micro\"], tpr_cnn[\"micro\"],\n",
    "        label=f'CNN (AUC = {roc_auc_cnn[\"micro\"]:.3f})', linewidth=2, color='red')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('Micro-Average ROC Curve: Random Forest vs CNN', fontsize=14)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gz10_roc_micro_average.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✅ Saved micro-average ROC curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "rf_misclassified = np.where(rf_predictions != y_val)[0]\n",
    "cnn_misclassified = np.where(predictions != true_labels)[0]\n",
    "\n",
    "print(f\"Random Forest misclassifications: {len(rf_misclassified)} / {len(y_val)} ({100*len(rf_misclassified)/len(y_val):.1f}%)\")\n",
    "print(f\"CNN misclassifications: {len(cnn_misclassified)} / {len(true_labels)} ({100*len(cnn_misclassified)/len(true_labels):.1f}%)\")\n",
    "\n",
    "# Find samples where RF was wrong but CNN was right\n",
    "rf_wrong_cnn_right = [i for i in rf_misclassified if i not in cnn_misclassified]\n",
    "# Find samples where CNN was wrong but RF was right\n",
    "cnn_wrong_rf_right = [i for i in cnn_misclassified if i not in rf_misclassified]\n",
    "# Find samples where both were wrong\n",
    "both_wrong = [i for i in rf_misclassified if i in cnn_misclassified]\n",
    "\n",
    "print(f\"\\nRF wrong, CNN right: {len(rf_wrong_cnn_right)}\")\n",
    "print(f\"CNN wrong, RF right: {len(cnn_wrong_rf_right)}\")\n",
    "print(f\"Both wrong: {len(both_wrong)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN misclassifications\n",
    "if len(cnn_misclassified) > 0:\n",
    "    print(\"Visualizing CNN misclassifications...\")\n",
    "    \n",
    "    # Show some misclassified examples\n",
    "    n_show = min(9, len(cnn_misclassified))\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        idx = cnn_misclassified[i]\n",
    "        img = X_val[idx]\n",
    "        true_label = true_labels[idx]\n",
    "        pred_label = predictions[idx]\n",
    "        rf_label = rf_predictions[idx]\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        title = f\"True: {class_names[true_label]}\\n\"\n",
    "        title += f\"CNN: {class_names[pred_label]}\\n\"\n",
    "        title += f\"RF: {class_names[rf_label]}\"\n",
    "        axes[i].set_title(title, fontsize=8, color='red')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('CNN Misclassifications (with RF predictions for comparison)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gz10_misclassified.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"✅ Saved misclassified examples\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary of Results:\n",
    "- Dataset: {len(gz10_data)} galaxies from Galaxy10 DECaLS\n",
    "- Classes: 10 morphological types\n",
    "- Training set: {len(X_train)} images\n",
    "- Validation set: {len(X_val)} images\n",
    "\n",
    "Random Forest Results:\n",
    "- Features: Color statistics, concentration, asymmetry\n",
    "- Training Accuracy: {rf_train_acc:.2f}%\n",
    "- Validation Accuracy: {rf_val_acc:.2f}%\n",
    "- Micro-average AUC: {roc_auc_rf['micro']:.3f}\n",
    "\n",
    "CNN Results:\n",
    "- Model: Simple CNN (~115K parameters)\n",
    "- Final Validation Accuracy: {final_acc:.2f}%\n",
    "- Micro-average AUC: {roc_auc_cnn['micro']:.3f}\n",
    "- Training epochs: {n_epochs}\n",
    "\n",
    "CNN Improvement over Random Forest:\n",
    "- Accuracy: +{final_acc - rf_val_acc:.1f} percentage points\n",
    "- AUC: +{roc_auc_cnn['micro'] - roc_auc_rf['micro']:.3f}\n",
    "\n",
    "Generated Files:\n",
    "- gz10_sample_galaxies.png\n",
    "- gz10_rf_performance.png\n",
    "- gz10_training_history.png\n",
    "- gz10_confusion_matrix.png\n",
    "- gz10_roc_curves.png\n",
    "- gz10_roc_micro_average.png\n",
    "- gz10_misclassified.png\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Project Ideas and Extensions\n",
    "\n",
    "Ideas to extend this analysis for your research project:\n",
    "\n",
    "### 1. Advanced Deep Learning\n",
    "- Use pre-trained models (ResNet, EfficientNet) with transfer learning\n",
    "- Implement attention mechanisms to highlight important features\n",
    "- Try Vision Transformers (ViT) for state-of-the-art performance\n",
    "\n",
    "### 2. Uncertainty Quantification\n",
    "- Implement Bayesian neural networks\n",
    "- Use dropout at test time for uncertainty estimates\n",
    "- Identify galaxies the model is uncertain about\n",
    "\n",
    "### 3. Rare Galaxy Discovery\n",
    "- Use anomaly detection to find unusual galaxies\n",
    "- Train on Galaxy Zoo to find rare types in gz10\n",
    "- Cross-match with catalogs to verify discoveries\n",
    "\n",
    "### 4. Morphology-Property Relations\n",
    "- Cross-match with SDSS for spectra\n",
    "- Study morphology vs star formation rate\n",
    "- Investigate environmental effects on morphology\n",
    "\n",
    "### 5. Domain Adaptation\n",
    "- Train on gz10, apply to other surveys (HSC, Legacy Survey)\n",
    "- Study how morphology classification transfers between surveys\n",
    "- Prepare for LSST classification\n",
    "\n",
    "### 6. Interpretable AI\n",
    "- Use GradCAM to visualize what the network sees\n",
    "- Extract learned features for scientific analysis\n",
    "- Compare CNN features with traditional morphology parameters\n",
    "\n",
    "### 7. Active Learning\n",
    "- Identify galaxies that would most improve the model\n",
    "- Design optimal training sets for new surveys\n",
    "- Human-in-the-loop classification systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
